{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0887959-1e39-4e58-b2e6-c71117fac7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "words = (open('names.txt','r')).read().splitlines()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1de2abe-5554-4e0f-84d0-820ea55f2afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "1a5a4b02-f405-4fbf-9a31-6c19c85e7c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 5, 13], [5, 13, 13]]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "def get(s):\n",
    "    return (ord(s) - ord('a')) + 1\n",
    "\n",
    "for word in words[:6]:\n",
    "    for i in range(len(word)):\n",
    "        now = [0, 0, 0]\n",
    "        now1 = 0\n",
    "        now[1] = get(word[i])\n",
    "        if(i - 1 >= 0):\n",
    "            now[0] = get(word[i-1])\n",
    "        if(i + 1 < len(word)):\n",
    "            now[2] = get(word[i+1])\n",
    "        if(i + 2 < len(word)):\n",
    "            now1 = get(word[i+2])\n",
    "        X.append(now)\n",
    "        Y.append(now1)\n",
    "\n",
    "print(X[:2])\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f7168d20-2666-47cb-88cf-eb4e83ce2aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uff, we have the dataset, now we make preparations\n",
    "\n",
    "C = torch.randn((27,2)) # this is the lookup table\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f520f324-ddf9-48a2-8dc4-08f1dd83d65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 6])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to prepare it for processing by the hidden layer\n",
    "# so we need to....?\n",
    "# we need to linearise the data C[X] so that it is easier to play with\n",
    "# we convert [36,3,2] to [36,6] by just putting all three things linearly \n",
    "N = C[X].view(len(C[X]),-1)\n",
    "N.shape\n",
    "\n",
    "# I guess that's what was needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "121a02f5-68f8-4a04-8e20-4a04aee2fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 6])\n",
      "torch.Size([6, 100])\n",
      "tensor([[ 1.0000,  1.0000, -0.9598,  ...,  0.8787,  1.0000,  1.0000],\n",
      "        [-0.5963,  0.9999, -1.0000,  ..., -0.8290,  1.0000,  0.8642],\n",
      "        [ 0.9952,  0.9988, -0.9556,  ..., -0.8418,  0.9998,  0.9936],\n",
      "        ...,\n",
      "        [ 1.0000,  1.0000,  0.7519,  ..., -0.9890,  0.9914,  0.9983],\n",
      "        [ 0.9990,  0.8727, -0.9938,  ..., -0.9849,  0.7026, -0.9991],\n",
      "        [-0.6756,  0.9832, -1.0000,  ...,  0.8495,  0.9999, -0.9503]])\n"
     ]
    }
   ],
   "source": [
    "# The hidden layer just does a tanh and sends it forward to the softmax\n",
    "W1 = torch.randn((6,100))\n",
    "B1 = torch.randn(100)\n",
    "\n",
    "print(N.shape)\n",
    "print(W1.shape)\n",
    "N = torch.tanh(N@W1 + B1)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d1bf8979-97a8-432f-bda5-95938e209bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 100])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8ce89dcd-a371-49b7-a65c-c6ed57ce990b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4225e-04, 1.3125e-09, 1.1810e-13, 4.0611e-05, 1.0984e-10, 3.0606e-07,\n",
       "         3.6250e-17, 1.4218e-11, 3.5698e-05, 1.6315e-07, 4.1490e-05, 1.5073e-08,\n",
       "         4.2095e-03, 3.2935e-06, 2.0639e-09, 9.8549e-01, 3.3810e-05, 4.7484e-03,\n",
       "         1.2424e-07, 1.8625e-03, 4.4915e-14, 2.6650e-11, 1.1929e-04, 6.8486e-04,\n",
       "         1.8902e-13, 1.3260e-09, 2.4894e-03],\n",
       "        [2.8258e-09, 1.3929e-04, 4.2443e-08, 1.1681e-06, 1.5380e-12, 5.0350e-04,\n",
       "         2.9769e-07, 6.9142e-08, 4.0397e-10, 2.6508e-08, 2.7211e-04, 3.9985e-10,\n",
       "         7.3642e-08, 2.1977e-06, 1.4239e-04, 2.2644e-04, 1.3310e-06, 2.8950e-04,\n",
       "         1.4664e-16, 9.5475e-03, 2.7588e-02, 5.7671e-15, 5.0751e-07, 9.6127e-01,\n",
       "         1.9043e-05, 1.8629e-08, 9.0785e-09],\n",
       "        [2.6299e-05, 1.4814e-08, 3.9049e-07, 1.7608e-03, 3.5016e-07, 9.7387e-02,\n",
       "         5.3876e-11, 1.7332e-02, 3.0113e-06, 1.2301e-08, 1.5596e-03, 9.0955e-10,\n",
       "         1.2664e-10, 3.0415e-07, 2.9839e-11, 2.3103e-07, 5.0240e-01, 1.9873e-07,\n",
       "         2.5003e-15, 4.5901e-12, 1.5548e-01, 5.1404e-03, 3.8856e-06, 4.3131e-08,\n",
       "         7.7468e-07, 2.1876e-01, 1.4546e-04],\n",
       "        [6.8882e-06, 6.9632e-10, 1.0531e-09, 1.1453e-02, 4.9085e-10, 4.9529e-08,\n",
       "         1.2759e-14, 1.7229e-06, 4.7265e-04, 5.1816e-09, 1.9799e-03, 2.0101e-09,\n",
       "         2.8205e-02, 2.4526e-03, 5.6196e-13, 6.4518e-02, 1.3378e-06, 6.4600e-01,\n",
       "         1.0381e-04, 2.4948e-03, 5.3766e-08, 4.5566e-07, 5.1035e-05, 2.0465e-01,\n",
       "         4.6644e-10, 1.0679e-02, 2.6931e-02],\n",
       "        [1.6901e-05, 2.1028e-03, 1.9571e-09, 6.4271e-06, 3.3650e-06, 9.7938e-07,\n",
       "         3.6349e-07, 7.8529e-10, 8.0372e-11, 6.1659e-11, 7.5568e-04, 6.6086e-07,\n",
       "         8.5666e-12, 1.6315e-04, 3.5090e-07, 6.7889e-04, 2.2541e-02, 3.8669e-04,\n",
       "         1.6584e-07, 8.7559e-04, 7.5356e-02, 2.3450e-06, 5.7083e-03, 4.9320e-04,\n",
       "         6.8113e-08, 1.5028e-03, 8.8940e-01],\n",
       "        [1.3725e-05, 4.4427e-04, 3.4075e-10, 8.1466e-06, 2.0223e-08, 3.2148e-08,\n",
       "         1.7079e-09, 8.6486e-11, 5.9081e-10, 1.1257e-14, 4.2592e-04, 6.4691e-08,\n",
       "         2.2145e-09, 1.3960e-02, 6.6459e-10, 1.9480e-05, 3.7559e-04, 1.8086e-02,\n",
       "         6.0241e-04, 1.3283e-03, 3.9602e-05, 2.5601e-10, 1.2831e-04, 2.4130e-02,\n",
       "         3.8738e-07, 1.4268e-05, 9.4042e-01],\n",
       "        [4.8865e-09, 1.5153e-02, 9.7757e-03, 4.6165e-12, 1.4561e-02, 2.2273e-10,\n",
       "         1.0522e-05, 1.6283e-09, 5.4400e-12, 1.8833e-15, 1.8114e-07, 3.7183e-04,\n",
       "         6.4752e-12, 1.0616e-04, 1.9725e-11, 4.6125e-13, 3.3712e-06, 9.3233e-09,\n",
       "         2.2028e-05, 7.7479e-05, 2.7054e-02, 6.5628e-08, 1.0061e-05, 4.7303e-07,\n",
       "         4.4150e-01, 4.9136e-01, 4.5818e-08],\n",
       "        [6.0287e-09, 1.3088e-08, 6.3458e-11, 6.3487e-10, 3.7430e-09, 3.9455e-19,\n",
       "         9.1380e-13, 2.8908e-13, 2.7487e-09, 1.7506e-18, 2.0490e-13, 9.3106e-10,\n",
       "         3.5179e-08, 1.2719e-06, 1.1757e-18, 8.9510e-12, 4.7772e-17, 3.6613e-07,\n",
       "         9.9457e-01, 5.4313e-03, 3.6105e-15, 7.6572e-15, 5.8188e-15, 1.1070e-06,\n",
       "         2.3120e-08, 1.8292e-09, 4.1449e-12],\n",
       "        [1.4740e-11, 6.0474e-01, 9.0054e-08, 4.2416e-12, 6.8183e-10, 7.5751e-12,\n",
       "         7.8645e-08, 2.6110e-15, 3.0244e-14, 5.4489e-13, 8.6748e-11, 1.3091e-08,\n",
       "         1.2612e-06, 9.7462e-03, 2.0093e-10, 6.1944e-13, 5.5713e-09, 2.5853e-10,\n",
       "         1.0786e-08, 3.8512e-01, 1.5315e-09, 1.0973e-15, 9.8395e-12, 1.7215e-07,\n",
       "         3.9028e-04, 5.2753e-13, 6.3593e-13],\n",
       "        [5.9568e-09, 7.2636e-09, 1.3866e-10, 6.8314e-09, 1.4062e-10, 1.3070e-11,\n",
       "         1.4796e-13, 3.3152e-08, 1.2422e-09, 9.5387e-16, 1.3368e-02, 1.0708e-09,\n",
       "         1.6288e-06, 9.3014e-04, 6.8077e-13, 1.6688e-06, 2.0630e-06, 7.7313e-05,\n",
       "         3.1028e-05, 6.1763e-05, 1.2559e-15, 7.4256e-16, 5.0489e-09, 9.8551e-01,\n",
       "         2.5819e-09, 4.9820e-07, 1.0898e-05],\n",
       "        [2.4052e-03, 2.5104e-03, 1.5065e-02, 7.0370e-07, 4.7607e-02, 1.8863e-09,\n",
       "         1.7643e-05, 9.7419e-05, 8.5976e-09, 2.3143e-13, 2.3934e-05, 1.1779e-02,\n",
       "         1.7713e-08, 2.9527e-02, 4.6719e-10, 5.7240e-09, 4.4140e-01, 5.4295e-03,\n",
       "         4.0424e-01, 7.7200e-05, 1.2575e-04, 4.6414e-06, 2.9951e-02, 3.4738e-05,\n",
       "         6.0678e-03, 3.8884e-05, 3.5967e-03],\n",
       "        [3.8288e-08, 8.3689e-03, 7.9123e-06, 8.2006e-10, 5.0003e-07, 2.0006e-16,\n",
       "         3.7738e-10, 9.9399e-11, 4.0854e-06, 9.3706e-14, 8.4667e-11, 1.8487e-09,\n",
       "         4.4727e-03, 2.3169e-07, 7.8450e-15, 3.0965e-09, 2.7792e-13, 8.8922e-07,\n",
       "         9.8640e-01, 3.6281e-04, 3.0856e-08, 2.2003e-12, 1.7682e-12, 1.7618e-05,\n",
       "         2.4495e-05, 3.4303e-04, 7.1849e-12],\n",
       "        [6.8197e-11, 5.0805e-07, 6.4258e-09, 1.6785e-15, 3.0893e-11, 2.8409e-18,\n",
       "         1.1998e-10, 1.0138e-14, 1.2855e-17, 4.8567e-13, 5.1131e-09, 9.4510e-10,\n",
       "         1.3097e-08, 1.5941e-04, 1.6970e-11, 1.3668e-11, 1.3423e-10, 5.6143e-13,\n",
       "         2.0075e-12, 9.9972e-01, 2.0296e-17, 2.4214e-19, 4.0326e-10, 1.2477e-04,\n",
       "         5.7008e-10, 4.7658e-13, 5.4960e-11],\n",
       "        [9.3526e-06, 1.0432e-02, 7.2472e-08, 5.4414e-01, 1.2931e-05, 8.2305e-08,\n",
       "         7.7740e-09, 1.2108e-05, 3.9335e-07, 8.8582e-11, 9.4560e-02, 1.1714e-07,\n",
       "         3.5832e-08, 1.6087e-01, 3.3872e-07, 4.1793e-04, 1.2228e-01, 2.2864e-02,\n",
       "         1.4056e-02, 2.7857e-04, 1.1592e-05, 1.5976e-04, 2.9788e-02, 6.1898e-06,\n",
       "         7.5173e-08, 8.9134e-05, 1.9187e-06],\n",
       "        [1.0987e-04, 4.2828e-06, 2.7221e-07, 3.7459e-07, 9.8586e-05, 3.0979e-06,\n",
       "         2.6820e-08, 9.5790e-09, 1.4926e-10, 2.5316e-09, 2.0031e-04, 2.0763e-07,\n",
       "         4.1480e-05, 1.5771e-03, 2.5858e-05, 3.8374e-09, 4.9869e-01, 4.8584e-06,\n",
       "         1.5442e-09, 2.4734e-01, 5.8737e-08, 1.6227e-13, 2.1699e-01, 3.4661e-02,\n",
       "         2.4589e-05, 5.3186e-09, 2.3290e-04],\n",
       "        [3.8742e-05, 2.8155e-04, 1.7508e-06, 8.1355e-04, 3.2572e-11, 2.3513e-08,\n",
       "         8.7701e-13, 4.0142e-02, 7.4510e-02, 2.6787e-09, 1.7771e-04, 6.4735e-14,\n",
       "         1.9461e-01, 6.1440e-04, 8.0574e-09, 1.4532e-04, 5.9809e-08, 2.1926e-01,\n",
       "         2.0379e-05, 1.4261e-05, 5.9316e-05, 1.9874e-06, 9.9020e-05, 4.6915e-01,\n",
       "         4.1298e-10, 5.6680e-05, 1.1290e-09],\n",
       "        [2.6959e-05, 1.5183e-01, 3.2876e-05, 2.8587e-06, 1.7588e-05, 6.9539e-07,\n",
       "         2.3668e-04, 6.3948e-08, 4.6354e-08, 6.5460e-06, 6.2020e-04, 4.6750e-11,\n",
       "         2.3798e-03, 6.8013e-01, 1.6665e-02, 1.5840e-09, 7.1568e-02, 7.0993e-06,\n",
       "         1.3527e-09, 6.4069e-04, 2.5993e-04, 2.4141e-10, 7.1000e-02, 3.1812e-03,\n",
       "         1.3887e-03, 1.4546e-07, 1.2757e-11],\n",
       "        [4.8797e-06, 3.2139e-09, 1.8612e-07, 1.8809e-05, 8.9478e-10, 6.8251e-08,\n",
       "         3.7344e-12, 6.6498e-03, 1.3650e-02, 6.9413e-11, 8.8442e-05, 1.8451e-11,\n",
       "         1.0014e-01, 1.0288e-03, 1.6080e-09, 6.1261e-09, 8.7787e-06, 2.0857e-01,\n",
       "         5.9119e-07, 4.9949e-08, 1.4893e-11, 4.5186e-11, 2.1479e-03, 6.6762e-01,\n",
       "         2.6565e-08, 7.6153e-05, 8.4241e-09],\n",
       "        [2.9251e-07, 5.9922e-01, 1.1493e-03, 1.7383e-09, 1.6322e-07, 2.3655e-07,\n",
       "         1.3935e-07, 5.7976e-07, 1.9307e-07, 2.2981e-13, 6.0083e-08, 3.0657e-13,\n",
       "         4.4433e-10, 1.6001e-02, 1.0004e-08, 4.0618e-08, 4.2125e-11, 7.4770e-03,\n",
       "         4.5809e-07, 1.1036e-03, 1.0020e-01, 1.1355e-11, 6.7239e-10, 2.6629e-01,\n",
       "         5.4561e-04, 8.0121e-03, 1.2923e-10],\n",
       "        [6.0141e-07, 9.9111e-01, 8.0966e-07, 3.3098e-08, 7.3987e-06, 3.9701e-10,\n",
       "         1.8828e-07, 1.0910e-10, 4.3557e-10, 1.6805e-15, 8.6673e-07, 1.1286e-11,\n",
       "         5.7773e-12, 5.9552e-03, 7.1028e-10, 3.0157e-08, 1.2284e-06, 3.8183e-07,\n",
       "         1.0898e-05, 2.5667e-03, 2.4297e-04, 2.0998e-12, 3.9036e-05, 4.7103e-05,\n",
       "         1.2912e-05, 1.1215e-06, 4.7215e-08],\n",
       "        [4.9623e-09, 2.8073e-09, 5.3037e-10, 2.3717e-08, 8.8794e-12, 3.8162e-11,\n",
       "         2.0789e-14, 1.4090e-07, 7.1699e-09, 5.5824e-16, 8.2160e-03, 1.3157e-10,\n",
       "         1.9850e-06, 5.0581e-04, 1.4939e-13, 5.4815e-06, 4.1565e-06, 3.4778e-04,\n",
       "         3.7437e-05, 1.6316e-05, 2.9042e-15, 5.1552e-15, 7.4436e-09, 9.9085e-01,\n",
       "         3.6933e-10, 5.9153e-07, 9.6361e-06],\n",
       "        [1.2871e-05, 2.6002e-09, 4.6786e-11, 2.5084e-04, 1.1213e-09, 7.6215e-05,\n",
       "         1.7699e-12, 2.4141e-11, 4.6471e-08, 3.7466e-10, 1.8982e-04, 4.7980e-06,\n",
       "         2.7774e-06, 5.9739e-02, 4.9482e-09, 7.5609e-05, 4.6283e-01, 3.9970e-04,\n",
       "         2.9899e-10, 6.2781e-06, 1.5670e-09, 3.2803e-12, 1.5411e-07, 4.5906e-01,\n",
       "         4.3000e-06, 1.7889e-07, 1.7342e-02],\n",
       "        [2.0721e-07, 5.1393e-06, 4.8077e-02, 1.7231e-03, 1.5990e-07, 7.9211e-06,\n",
       "         1.7483e-11, 4.7271e-04, 8.6363e-07, 1.0169e-11, 1.2147e-02, 2.4021e-12,\n",
       "         6.0830e-09, 2.3199e-06, 1.5544e-09, 6.4779e-03, 2.1107e-07, 4.0912e-02,\n",
       "         1.7102e-07, 1.5919e-02, 5.8780e-01, 1.3477e-10, 5.2794e-08, 2.8615e-01,\n",
       "         1.0755e-07, 2.9491e-04, 4.8132e-06],\n",
       "        [7.2432e-07, 5.4169e-04, 1.4671e-06, 4.2962e-08, 2.7152e-04, 2.2546e-05,\n",
       "         7.0867e-05, 4.4271e-07, 1.0302e-12, 1.4823e-11, 5.0791e-03, 3.3534e-07,\n",
       "         4.0889e-13, 1.2712e-02, 2.0148e-05, 1.3241e-06, 8.3992e-01, 4.1067e-06,\n",
       "         3.3691e-09, 1.8608e-05, 7.4740e-02, 5.6960e-05, 6.1644e-02, 3.7396e-07,\n",
       "         3.4267e-07, 1.6889e-03, 3.2031e-03],\n",
       "        [3.3824e-06, 6.0777e-10, 1.3964e-08, 3.3651e-02, 1.7427e-10, 2.2399e-10,\n",
       "         3.0357e-14, 2.1215e-07, 4.7924e-07, 1.0275e-14, 8.3867e-05, 2.3318e-10,\n",
       "         2.1379e-06, 9.4004e-03, 2.6167e-15, 1.5372e-06, 7.1657e-08, 8.4244e-01,\n",
       "         8.2051e-03, 2.5046e-05, 2.5697e-09, 3.2126e-08, 2.4644e-04, 1.0563e-01,\n",
       "         2.8512e-09, 1.7429e-06, 3.1644e-04],\n",
       "        [6.0120e-10, 9.9984e-01, 4.1614e-06, 1.2942e-12, 5.6011e-08, 7.8029e-12,\n",
       "         2.0943e-08, 7.1723e-14, 1.5441e-11, 1.5572e-14, 1.8745e-10, 5.4193e-12,\n",
       "         2.9804e-10, 1.7642e-05, 6.3814e-12, 5.4978e-13, 5.9718e-12, 2.5007e-09,\n",
       "         1.1432e-07, 6.0897e-05, 4.7693e-06, 4.6421e-16, 3.7350e-11, 2.2895e-07,\n",
       "         6.7481e-05, 1.7887e-08, 1.6458e-14],\n",
       "        [5.9568e-09, 7.2636e-09, 1.3866e-10, 6.8314e-09, 1.4062e-10, 1.3070e-11,\n",
       "         1.4796e-13, 3.3152e-08, 1.2422e-09, 9.5387e-16, 1.3368e-02, 1.0708e-09,\n",
       "         1.6288e-06, 9.3014e-04, 6.8077e-13, 1.6688e-06, 2.0630e-06, 7.7313e-05,\n",
       "         3.1028e-05, 6.1763e-05, 1.2559e-15, 7.4256e-16, 5.0489e-09, 9.8551e-01,\n",
       "         2.5819e-09, 4.9820e-07, 1.0898e-05],\n",
       "        [1.2714e-05, 1.3624e-04, 3.2977e-02, 1.6463e-04, 2.7287e-01, 3.2739e-09,\n",
       "         2.2680e-07, 3.0950e-08, 1.7295e-12, 3.9575e-18, 1.7400e-07, 2.6660e-08,\n",
       "         1.9554e-15, 1.1635e-06, 4.6649e-12, 5.4664e-06, 2.6190e-07, 1.1388e-06,\n",
       "         2.4721e-02, 4.1646e-07, 6.6880e-01, 1.0108e-04, 4.3166e-07, 4.5831e-10,\n",
       "         3.4100e-06, 1.9581e-04, 1.8676e-07],\n",
       "        [7.2457e-06, 1.0098e-03, 1.2998e-11, 6.2230e-09, 6.7490e-06, 1.1070e-13,\n",
       "         1.4453e-07, 8.2202e-14, 2.5343e-14, 9.3841e-13, 5.4638e-09, 2.9810e-05,\n",
       "         7.0013e-08, 3.9564e-06, 1.3725e-06, 1.8573e-08, 2.0916e-04, 1.2885e-08,\n",
       "         1.5450e-03, 9.3207e-01, 6.2609e-12, 8.7553e-11, 4.5391e-02, 5.2928e-06,\n",
       "         5.1718e-08, 2.2098e-13, 1.9723e-02],\n",
       "        [3.5846e-10, 1.0000e+00, 8.9704e-13, 1.0844e-12, 2.4646e-12, 4.9053e-15,\n",
       "         5.6605e-11, 1.3268e-13, 7.3066e-13, 3.8710e-10, 2.5811e-11, 2.3396e-13,\n",
       "         4.0513e-09, 1.3516e-09, 3.5022e-13, 5.3073e-14, 4.5515e-10, 2.0246e-14,\n",
       "         5.0580e-14, 5.0327e-11, 5.9212e-12, 5.5014e-12, 1.4701e-11, 4.9502e-12,\n",
       "         3.1798e-11, 3.1597e-13, 6.1252e-17],\n",
       "        [1.8940e-07, 2.7942e-08, 7.6180e-10, 5.3339e-09, 5.7084e-10, 1.7728e-10,\n",
       "         1.3591e-11, 3.5466e-06, 1.6922e-10, 7.0227e-07, 1.1558e-06, 3.9031e-10,\n",
       "         1.3202e-01, 5.0503e-03, 1.8624e-07, 1.6052e-12, 2.9757e-04, 7.1300e-07,\n",
       "         8.2307e-11, 1.6101e-03, 1.9735e-15, 2.7374e-16, 4.3833e-01, 4.2269e-01,\n",
       "         2.7594e-09, 1.3178e-10, 4.5877e-08],\n",
       "        [2.4830e-06, 2.4180e-05, 3.6708e-04, 6.2269e-08, 1.1960e-08, 7.9242e-07,\n",
       "         2.3300e-09, 4.8884e-06, 7.3756e-06, 6.8161e-13, 8.9636e-10, 1.8204e-13,\n",
       "         3.1492e-10, 1.4623e-03, 2.5488e-12, 2.0971e-06, 1.8600e-13, 4.8028e-01,\n",
       "         8.0302e-08, 2.9547e-04, 1.3316e-02, 6.1923e-09, 2.8479e-11, 4.7717e-01,\n",
       "         1.6024e-05, 2.7058e-02, 8.5478e-11],\n",
       "        [1.1035e-07, 9.9739e-01, 4.6449e-08, 8.4651e-05, 5.1081e-08, 6.2774e-10,\n",
       "         8.6493e-06, 5.4331e-09, 1.8049e-08, 7.3712e-11, 2.8064e-06, 4.2026e-11,\n",
       "         2.0877e-10, 5.1048e-06, 2.0997e-04, 6.4055e-07, 4.7254e-04, 8.9257e-08,\n",
       "         1.5018e-10, 3.4305e-08, 1.7930e-03, 9.7788e-09, 2.8605e-05, 1.0494e-07,\n",
       "         3.6608e-08, 6.9728e-08, 1.2903e-10],\n",
       "        [1.3871e-06, 2.0525e-05, 1.3625e-11, 9.0134e-05, 2.1297e-11, 3.4361e-05,\n",
       "         1.3846e-10, 1.3950e-07, 3.7609e-06, 3.3662e-04, 1.1773e-05, 1.5504e-08,\n",
       "         3.3203e-04, 2.8336e-07, 5.0312e-03, 1.2230e-07, 9.9342e-01, 3.9780e-07,\n",
       "         1.8978e-11, 1.0192e-06, 5.4631e-08, 8.9238e-13, 7.1507e-04, 3.4496e-08,\n",
       "         5.8930e-11, 2.4222e-09, 1.9860e-06],\n",
       "        [2.7684e-05, 8.9294e-05, 2.1107e-06, 5.9022e-08, 1.7173e-08, 3.9951e-06,\n",
       "         2.9986e-09, 2.3825e-04, 8.2539e-02, 1.9087e-04, 2.1929e-04, 1.1628e-10,\n",
       "         1.5702e-02, 3.9565e-03, 8.6806e-01, 2.3525e-10, 6.0220e-03, 1.1801e-03,\n",
       "         2.2693e-10, 1.1484e-08, 2.3170e-05, 2.0482e-11, 2.0652e-02, 1.4347e-04,\n",
       "         9.3981e-04, 5.7671e-06, 1.9793e-10],\n",
       "        [9.6519e-07, 1.6661e-09, 2.6667e-05, 7.9149e-07, 7.9735e-11, 2.3312e-07,\n",
       "         2.2648e-16, 5.0060e-04, 5.2441e-03, 5.4743e-11, 2.3133e-05, 5.5052e-13,\n",
       "         1.7665e-02, 3.4743e-06, 9.9131e-09, 9.1281e-07, 7.5489e-07, 9.3868e-01,\n",
       "         4.8381e-08, 1.0565e-04, 5.4858e-09, 2.3287e-11, 2.9295e-05, 3.7669e-02,\n",
       "         5.7254e-10, 4.7315e-05, 6.7213e-08]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax layer, we get [36,100] from last layer\n",
    "# we must return a single layer of size 26 with the probabilities of all chars\n",
    "W2 = torch.randn((100,27)) \n",
    "B2 = torch.randn(27)\n",
    "\n",
    "logits = N@W2 + B2\n",
    "counts = logits.exp() # remove the negative logits\n",
    "probs = counts / counts.sum(1,keepdims=True)\n",
    "\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "53c1aebf-1ade-41d7-af37-358684ae9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have the final probabilities for each input?, oh wait, its all random right now, we need to train it\n",
    "# but how do \n",
    "# the idea is: with a neural network, we are just processing the probabilities, we get what we put in\n",
    "# its totally random right now, but I need to train it\n",
    "# but how do I get the output of the things I put in ?\n",
    "# we have already put in the things that we need to find the probablities of\n",
    "# how th find em ?\n",
    "# the things is, we need to find the log likelihood of the expected ouputs with a given input\n",
    "# and we just have to optimize for that\n",
    "# okayyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8adaa711-19e3-46a2-847d-5ae54e3c1f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.0706)\n"
     ]
    }
   ],
   "source": [
    "# find out log likelihood for the expected output\n",
    "\n",
    "def get_loss():\n",
    "    yo = probs[torch.arange(len(Y)),Y].log().mean()\n",
    "    #print(yo)\n",
    "    \n",
    "    likelihood = 0\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        pro = probs[i,Y[i]]\n",
    "        likelihood += torch.log(pro)\n",
    "    \n",
    "    likelihood /= len(X)\n",
    "    \n",
    "    #print(likelihood)\n",
    "    \n",
    "    loss = -likelihood\n",
    "    print(loss)\n",
    "\n",
    "\n",
    "get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "2b589637-93b6-4c39-b5ef-5c5ae291c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we just do gradient descent to optimize weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eddb3b6-4673-469a-bd6a-533ae000794e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
